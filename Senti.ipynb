{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a15efc60",
   "metadata": {},
   "source": [
    "# Sentiment Analysis with Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29476f99",
   "metadata": {},
   "source": [
    "## Import the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4ffb6704-685f-43f9-b4eb-94d90a0218fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to /Users/danc/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     /Users/danc/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker_tab to\n",
      "[nltk_data]     /Users/danc/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker_tab is already up-to-date!\n",
      "[nltk_data] Downloading package words to /Users/danc/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/danc/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from typing import List, Dict\n",
    "import math\n",
    "from tqdm.notebook import tqdm\n",
    "from scipy.special import softmax\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datasets import load_dataset_builder, load_dataset, get_dataset_split_names, concatenate_datasets\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "import nltk\n",
    "\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "nltk.download('maxent_ne_chunker_tab')\n",
    "nltk.download('words')\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "775b1ac1",
   "metadata": {},
   "source": [
    "## Check the features of the IMDb dataset, then load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f792d58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': Value(dtype='string', id=None),\n",
       " 'label': ClassLabel(names=['neg', 'pos'], id=None)}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_builder = load_dataset_builder(\"imdb\")\n",
    "\n",
    "# Inspect dataset features\n",
    "ds_builder.info.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e185d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the IMDb dataset from Hugging Face\n",
    "imdb = load_dataset(\"imdb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1073eeca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['train', 'test', 'unsupervised']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the dataset split names\n",
    "get_dataset_split_names(\"imdb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "efadbb81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    unsupervised: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 50000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the dataset\n",
    "imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1b1f6d59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. I also heard that at first it was seized by U.S. customs if it ever tried to enter this country, therefore being a fan of films considered \"controversial\" I really had to see this for myself.<br /><br />The plot is centered around a young Swedish drama student named Lena who wants to learn everything she can about life. In particular she wants to focus her attentions to making some sort of documentary on what the average Swede thought about certain political issues such as the Vietnam War and race issues in the United States. In between asking politicians and ordinary denizens of Stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men.<br /><br />What kills me about I AM CURIOUS-YELLOW is that 40 years ago, this was considered pornographic. Really, the sex and nudity scenes are few and far between, even then it\\'s not shot like some cheaply made porno. While my countrymen mind find it shocking, in reality sex and nudity are a major staple in Swedish cinema. Even Ingmar Bergman, arguably their answer to good old boy John Ford, had sex scenes in his films.<br /><br />I do commend the filmmakers for the fact that any sex shown in the film is shown for artistic purposes rather than just to shock people and make money to be shown in pornographic theaters in America. I AM CURIOUS-YELLOW is a good film for anyone wanting to study the meat and potatoes (no pun intended) of Swedish cinema. But really, this film doesn\\'t have much of a plot.',\n",
       " 'label': 0}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the training dataset\n",
    "ds_train = imdb['train']\n",
    "ds_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2847210a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'I love sci-fi and am willing to put up with a lot. Sci-fi movies/TV are usually underfunded, under-appreciated and misunderstood. I tried to like this, I really did, but it is to good TV sci-fi as Babylon 5 is to Star Trek (the original). Silly prosthetics, cheap cardboard sets, stilted dialogues, CG that doesn\\'t match the background, and painfully one-dimensional characters cannot be overcome with a \\'sci-fi\\' setting. (I\\'m sure there are those of you out there who think Babylon 5 is good sci-fi TV. It\\'s not. It\\'s clichéd and uninspiring.) While US viewers might like emotion and character development, sci-fi is a genre that does not take itself seriously (cf. Star Trek). It may treat important issues, yet not as a serious philosophy. It\\'s really difficult to care about the characters here as they are not simply foolish, just missing a spark of life. Their actions and reactions are wooden and predictable, often painful to watch. The makers of Earth KNOW it\\'s rubbish as they have to always say \"Gene Roddenberry\\'s Earth...\" otherwise people would not continue watching. Roddenberry\\'s ashes must be turning in their orbit as this dull, cheap, poorly edited (watching it without advert breaks really brings this home) trudging Trabant of a show lumbers into space. Spoiler. So, kill off a main character. And then bring him back as another actor. Jeeez! Dallas all over again.',\n",
       " 'label': 0}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the test dataset\n",
    "ds_test = imdb['test']\n",
    "ds_test[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d8ad6f73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'I love sci-fi and am willing to put up with a lot. Sci-fi movies/TV are usually underfunded, under-appreciated and misunderstood. I tried to like this, I really did, but it is to good TV sci-fi as Babylon 5 is to Star Trek (the original). Silly prosthetics, cheap cardboard sets, stilted dialogues, CG that doesn\\'t match the background, and painfully one-dimensional characters cannot be overcome with a \\'sci-fi\\' setting. (I\\'m sure there are those of you out there who think Babylon 5 is good sci-fi TV. It\\'s not. It\\'s clichéd and uninspiring.) While US viewers might like emotion and character development, sci-fi is a genre that does not take itself seriously (cf. Star Trek). It may treat important issues, yet not as a serious philosophy. It\\'s really difficult to care about the characters here as they are not simply foolish, just missing a spark of life. Their actions and reactions are wooden and predictable, often painful to watch. The makers of Earth KNOW it\\'s rubbish as they have to always say \"Gene Roddenberry\\'s Earth...\" otherwise people would not continue watching. Roddenberry\\'s ashes must be turning in their orbit as this dull, cheap, poorly edited (watching it without advert breaks really brings this home) trudging Trabant of a show lumbers into space. Spoiler. So, kill off a main character. And then bring him back as another actor. Jeeez! Dallas all over again.',\n",
       " 'label': 0}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Concatenate the training and test datasets\n",
    "ds = concatenate_datasets([ds_train, ds_test])\n",
    "ds[25000]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0d1be477",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'Boris Karloff and Bela Lugosi made many films together, but on the whole (interestingly enough) Karloff usually is the better man of the two. The real exception is \"The Black Cat\" (1934) where Karloff is playing the evil head of a devil cult, and Lugosi is seeking revenge on him for destroying his life. But more usual is \"Black Friday\", where (whatever his motive) Karloff is trying to improve brain surgery while Lugosi is a murderous thug. In \"The Raven\" Lugosi is a sadistic surgeon, who blackmails Karloff to assist his evil plans until Karloff finally has had enough. Rarely are they both negative characters totally. In \"The Body Snatcher\", Karloff does kill Lugosi, but Lugosi is trying to blackmail him.<br /><br />The one exception where they are both extremely sympathetic but at cross purposes to each other is this 1936 film, which I feel has rarely had the audience acceptance of some of the other movies I have mentioned. In it Karloff\\'s Dr. Janos Rukh is a hard driven scientific genius who has been sneered at by the \"official scientific community\" for his theory that a rare form of Radium is in Nigeria on a meteorite that landed centuries ago. He has finally gotten the support of a well financed expedition led by Sir Francis Stevens and his wife Lady Arabella Stevens (Walter Kingsford and Beulah Bondi), and has another scientist, a Frenchman named Dr. Felix Benet (Lugosi), Rukh\\'s young wife Diane (Frances Drake) and a friend and protégée of the Stevenses named Ronald Drake (Frank Lawton).<br /><br />Before they leave, Rukh is warned by his mother (Violet Kemble Cooper) that he is possibly seeking wisdom that he shouldn\\'t and it may end in tragedy. He tries to dismiss this, but he is worried by what she says, his scientific standing, and whether or not he is going to get his due credit.<br /><br />What he gets is a disaster. He finds the substance, but is infected by it\\'s remarkable radioactivity. He finds that he is slowly burning up, and if he tries to touch people or animals they die. He\\'s actually built up a friendship or understanding with Benet, who figures out a type of radioactive fighting cocktail for Rukh to use to counter the danger. But there are two things that are unbeatable here. The antidote can only last for a certain amount of time, and has to be replenished. And the radioactivity has affected Rukh\\'s brain. He is increasingly jealous of Diane\\'s friendship with Ronald (encouraged, unfortunately by Sir Francis and Lady Arabella), and he is equally upset that (due to his having to pretend to have died - the effects of the radioactivity are like that) Benet and several others are collecting the kudos of the wonders that \"Radium X\" is giving to man. Soon Rukh is on a murderous rampage that destroys many lives, ending with his own.<br /><br />The film certainly picked up on science to an extent. Madame Curie had died recently from cancer she got due to work with Radium. Few fully understood the dangers of radioactivity in 1936, but some idea of it was coming out. The wave of murders by Rukh cause the newspapers to talk about a \"curse\" on the expedition. Of course, with the idea of a \"cursed\" expedition (on the continent of Africa) for a hidden treasure buried centuries ago, financed by a titled Englishman, we have entered archeology not physics or geology (paging Howard Carter and Lord Carnaevon).<br /><br />On the other hand, Benet tries to settle the cause of the string of deaths, and reverts to an idea that was actually demolished in 1888 in England. During the Whitechapel Murders, Sir Charles Warren ordered the retinas of several of the dead victims to be photographed to see if the last image on the retinas was Jack the Ripper. It turned out he only got the photographs of the retinas of dead prostitutes. But the idea did not die. Jules Verne used it in his novel \"The Brothers Kip\" in 1899, and here Dr. Benet uses it. As this is a science fiction story, he finds the image of Rukh on the the plate, but Benet drops the plate accidentally and it shatters.<br /><br />The film is good on many grounds, the most interesting that for a change Karloff and Lugosi are not unsympathetic towards each other. There is a type of tragic fatalism in this story that is missing from their other films. The other performances are good as well, in particular Ms Kemble Cooper. She is best remembered as Basil Rathbone\\'s frightening sister (Jane Murdstone) in \"David Copperfield\". Here her final act is the only way to bring this tragedy to an end, and who can say it did not hurt her more than her target.',\n",
       " 'label': 1}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shuffle the dataset\n",
    "shuffled_dataset = ds.shuffle(seed=10)\n",
    "shuffled_dataset[25000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "662a12cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': \"The film is worth watching only if you stop it after half an hour. It starts of with funny conversations in a bar and makes one expect a good, funny story is to come. Well, I can tell you it will not come. It will deteriorate in minutes into a movie that challenges your patience as well as your feelings of shame for the actors to an extend you will probably not be pleased to witness. <br /><br />In an interview I heard that the director wanted to express in this film the feeling of a loss of identity that, according to him, the majority of the people in this globalizing world experience. I was amazed to hear that. Am I living in the same world he lives in? OK a lot of people do walk around in the same clothes as mine and listen to the same music and all, but that doesn't make me feel like I am losing my identity. What does Khrzhanosvky think, that we are not more than the clothes we wear and the movies we watch? Am I shortsighted or is he?<br /><br />Well my vote: the good start of the movie saves it from getting a 1, a decent 4 is my conclusion.\",\n",
       " 'label': 0}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split the dataset into training, validation, and test sets\n",
    "ds_train = shuffled_dataset.select(range(30000))\n",
    "ds_val = shuffled_dataset.select(range(30000, 40000))\n",
    "ds_test = shuffled_dataset.select(range(40000, 50000))\n",
    "#ds_train[-1]\n",
    "#ds_val[-1]\n",
    "ds_test[-1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce7eff4",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5802adc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDA\n",
    "# check the distribution of the labels\n",
    "labels_train = pd.Series(ds_train['label'])\n",
    "ax = labels_train.value_counts().sort_index() \\\n",
    "    .plot(kind='bar',\n",
    "          title='Count of Reviews by Sentiment',\n",
    "          figsize=(10, 5))\n",
    "ax.set_xlabel('Sentiment')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77cde64",
   "metadata": {},
   "source": [
    "## Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7f24ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic NLTK preprocessing\n",
    "example_text = ds_train[0]['text']\n",
    "example_text\n",
    "tokens = nltk.word_tokenize(example_text)\n",
    "tagged = nltk.pos_tag(tokens)\n",
    "entities = nltk.chunk.ne_chunk(tagged)\n",
    "entities.pprint()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d81b336",
   "metadata": {},
   "source": [
    "## Model Building"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c02420",
   "metadata": {},
   "source": [
    "### Use the VADER model as a baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15fd8e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VADER (Valence Aware Dictionary and Sentiment Reasoner), ignoring contexts\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "sia.polarity_scores(example_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8bdcd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the polarity score on the entire training dataset\n",
    "results_train = {}\n",
    "counter = 0\n",
    "for row in tqdm(ds_train, total=len(ds_train)):\n",
    "    text = row['text']\n",
    "    results_train[counter] = sia.polarity_scores(text)\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6492a603",
   "metadata": {},
   "outputs": [],
   "source": [
    "vaders = pd.DataFrame(results_train).T\n",
    "vaders = vaders.reset_index().rename(columns={'index': 'Id'})\n",
    "vaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeded193",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.DataFrame(ds_train)\n",
    "df_train = df_train.reset_index().rename(columns={'index': 'Id'})\n",
    "vaders = vaders.merge(df_train, how='left')\n",
    "vaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e37ce80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot VADER scores by sentiment label\n",
    "ax = sns.barplot(data=vaders, x='label', y='compound')\n",
    "ax.set_title('Compound Score by Sentiment Label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369de4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot VADER sub-scores by sentiment label\n",
    "fig, axs = plt.subplots(1, 3, figsize=(12, 3))\n",
    "sns.barplot(data=vaders, x='label', y='pos', ax=axs[0])\n",
    "sns.barplot(data=vaders, x='label', y='neu', ax=axs[1])\n",
    "sns.barplot(data=vaders, x='label', y='neg', ax=axs[2])\n",
    "axs[0].set_title('Positive')\n",
    "axs[1].set_title('Neutral')\n",
    "axs[2].set_title('Negative')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ec69b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the Roberta pretrained model for sentiment analysis\n",
    "MODEL = f\"cardiffnlp/twitter-roberta-base-sentiment\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea0b775",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the example text through the Roberta model and get the scores\n",
    "encoded_text = tokenizer(example_text, return_tensors='pt')\n",
    "output = model(**encoded_text)\n",
    "scores = output[0][0].detach().numpy()\n",
    "scores = softmax(scores)\n",
    "scores_dict = {\n",
    "    'roberta_neg' : scores[0],\n",
    "    'roberta_neu' : scores[1],\n",
    "    'roberta_pos' : scores[2]\n",
    "}\n",
    "scores_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2dd7374b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a function to run the Roberta model on each entry\n",
    "def polarity_scores_roberta(example):\n",
    "    # Tokenize with truncation\n",
    "    encoded_text = tokenizer(\n",
    "        example, \n",
    "        return_tensors='pt', \n",
    "        truncation=True, \n",
    "        padding=True, \n",
    "        max_length=512\n",
    "    )\n",
    "    output = model(**encoded_text)\n",
    "    scores = output[0][0].detach().numpy()\n",
    "    scores = softmax(scores)\n",
    "    print(scores)\n",
    "    scores_dict = {\n",
    "        'roberta_neg': scores[0],\n",
    "        'roberta_neu': scores[1],\n",
    "        'roberta_pos': scores[2]\n",
    "    }\n",
    "    return scores_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c4cfcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the function on the example text\n",
    "polarity_scores_roberta(example_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912c5386",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the Roberta model on the training dataset\n",
    "results_train_both = {}\n",
    "counter = 0\n",
    "\n",
    "for row in tqdm(ds_train, total=len(ds_train)):\n",
    "    try:\n",
    "        text = row['text']\n",
    "        vader_result = sia.polarity_scores(text)\n",
    "        vader_result_rename = {}\n",
    "        for key, value in vader_result.items():\n",
    "            vader_result_rename[f\"vader_{key}\"] = value\n",
    "        roberta_result = polarity_scores_roberta(text)\n",
    "        both = {**vader_result_rename, **roberta_result}\n",
    "        results_train_both[counter] = both\n",
    "        counter += 1\n",
    "    except RuntimeError:\n",
    "        print(f'Broke for row {counter}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23f7376",
   "metadata": {},
   "outputs": [],
   "source": [
    "# try to run the model on Mac GPUs\n",
    "\n",
    "#from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
    "\n",
    "# Check if MPS (Metal Performance Shaders) is available\n",
    "device = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ead1ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer and model\n",
    "model = model.to(device)\n",
    "\n",
    "# Tokenize the input\n",
    "inputs = tokenizer(example_text, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "\n",
    "# Run inference\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "\n",
    "# Get predictions\n",
    "predicted_class = torch.argmax(logits, dim=1).item()\n",
    "print(example_text)\n",
    "print(f\"Predicted class: {predicted_class}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2249bb28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a function to run the Roberta model on the GPUs\n",
    "def polarity_scores_roberta_gpu(example):\n",
    "    # Tokenize with truncation\n",
    "    encoded_text = tokenizer(\n",
    "        example, \n",
    "        return_tensors='pt', \n",
    "        truncation=True, \n",
    "        padding=True, \n",
    "        max_length=512\n",
    "    ).to(device)\n",
    "    \n",
    "    # Run inference\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**encoded_text)\n",
    "        #logits = outputs.logits\n",
    "\n",
    "    # Get predictions\n",
    "    scores = outputs[0][0].detach().cpu().numpy()\n",
    "    scores = softmax(scores)\n",
    "    #print(scores)\n",
    "    scores_dict = {\n",
    "        'roberta_neg': scores[0],\n",
    "        'roberta_neu': scores[1],\n",
    "        'roberta_pos': scores[2]\n",
    "    }\n",
    "    return scores_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af088ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the function on the example text\n",
    "polarity_scores_roberta_gpu(example_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d305f26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the Roberta model on the training dataset using GPUs\n",
    "results_train_both = {}\n",
    "counter = 0\n",
    "\n",
    "for row in tqdm(ds_train, total=len(ds_train)):\n",
    "    try:\n",
    "        text = row['text']\n",
    "        vader_result = sia.polarity_scores(text)\n",
    "        vader_result_rename = {}\n",
    "        for key, value in vader_result.items():\n",
    "            vader_result_rename[f\"vader_{key}\"] = value\n",
    "        roberta_result = polarity_scores_roberta_gpu(text)\n",
    "        both = {**vader_result_rename, **roberta_result}\n",
    "        results_train_both[counter] = both\n",
    "        counter += 1\n",
    "    except RuntimeError:\n",
    "        print(f'Broke for row {counter}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0306ea20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the Roberta model with batch input using GPUs\n",
    "def polarity_scores_roberta_gpu_batch(texts: List[str], batch_size: int = 256) -> Dict[int, Dict[str, float]]:\n",
    "    \"\"\"\n",
    "    Analyze sentiment for a batch of texts.\n",
    "        \n",
    "    Args:\n",
    "        texts (List[str]): List of input texts\n",
    "        batch_size (int): Size of the batch to process\n",
    "            \n",
    "    Returns:\n",
    "        Dict[int, Dict[str, float]]: Dictionary of dictionaries containing sentiment probabilities\n",
    "    \"\"\"\n",
    "\n",
    "    scores_dict = {}\n",
    "    num_batches = math.ceil(len(texts) / batch_size)\n",
    "         \n",
    "    for i in tqdm(range(num_batches)):\n",
    "        # Get the batch of texts\n",
    "        batch_texts = texts[i*batch_size : (i+1)*batch_size]\n",
    "        \n",
    "        # Tokenize with truncation\n",
    "        encoded_text = tokenizer(\n",
    "                 batch_texts, \n",
    "                 return_tensors='pt', \n",
    "                 truncation=True, \n",
    "                 padding=True, \n",
    "                 max_length=512\n",
    "             ).to(device)\n",
    "             \n",
    "        # Run inference\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**encoded_text)\n",
    "            probabilities = torch.nn.functional.softmax(outputs.logits, dim=1)\n",
    "             \n",
    "        # Get predictions\n",
    "        for j, probs in enumerate(probabilities.cpu().numpy()):\n",
    "            idx = i * batch_size + j\n",
    "            scores_dict[idx] = {\n",
    "                     \"roberta_neg\": float(probs[0]),\n",
    "                     \"roberta_neu\": float(probs[1]),\n",
    "                     \"roberta_pos\": float(probs[2])\n",
    "                 }\n",
    "         \n",
    "    return scores_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5739d9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "roberta_batch_result = polarity_scores_roberta_gpu_batch(ds_train['text'])\n",
    "roberta_batch_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5e95f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4adfcb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185ba75d",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(results_train_both).T\n",
    "results_df = results_df.reset_index().rename(columns={'index': 'Id'})\n",
    "results_df = results_df.merge(df_train, how='left')\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4bbbd4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare the VADER and Roberta scores\n",
    "sns.pairplot(data=results_df,\n",
    "             vars=['vader_neg', 'vader_neu', 'vader_pos',\n",
    "                  'roberta_neg', 'roberta_neu', 'roberta_pos'],\n",
    "            hue='label',\n",
    "            palette='tab10')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c390b55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check the VADER and Roberta scores\n",
    "results_df.query('label == 0') \\\n",
    "    .sort_values('roberta_pos', ascending=False)['text'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1abc6ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.query('label == 0') \\\n",
    "    .sort_values('vader_pos', ascending=False)['text'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b260a459",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.query('label == 1') \\\n",
    "    .sort_values('roberta_neg', ascending=False)['text'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91dc112",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.query('label == 1') \\\n",
    "    .sort_values('vader_neg', ascending=False)['text'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45d782e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the Transformers Pipeline for sentiment analysis\n",
    "from transformers import pipeline\n",
    "\n",
    "sent_pipeline = pipeline(\"sentiment-analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "004df64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_pipeline(example_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a1f356",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_pipeline('I love sentiment analysis!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943b6bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_pipeline('I hate having no gpus!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f06fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_pipeline('What a beautiful day! All my clothes got wet!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a80604",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a20803",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2c25d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomize the test set\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f93971",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split a validation set from the test set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476d6676",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
