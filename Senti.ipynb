{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a15efc60",
   "metadata": {},
   "source": [
    "# Sentiment Analysis with Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29476f99",
   "metadata": {},
   "source": [
    "## Import the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ffb6704-685f-43f9-b4eb-94d90a0218fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to /Users/danc/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     /Users/danc/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker_tab to\n",
      "[nltk_data]     /Users/danc/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker_tab is already up-to-date!\n",
      "[nltk_data] Downloading package words to /Users/danc/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/danc/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/danc/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from typing import List, Dict\n",
    "from collections import Counter\n",
    "import math\n",
    "from tqdm.notebook import tqdm\n",
    "from scipy.special import softmax\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datasets import load_dataset_builder, load_dataset, get_dataset_split_names, concatenate_datasets\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer \n",
    "\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "nltk.download('maxent_ne_chunker_tab')\n",
    "nltk.download('words')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "775b1ac1",
   "metadata": {},
   "source": [
    "## Check the features of the IMDb dataset, then load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f792d58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': Value(dtype='string', id=None),\n",
       " 'label': ClassLabel(names=['neg', 'pos'], id=None)}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_builder = load_dataset_builder(\"imdb\")\n",
    "\n",
    "# Inspect dataset features\n",
    "ds_builder.info.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e185d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the IMDb dataset from Hugging Face\n",
    "imdb = load_dataset(\"imdb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1073eeca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['train', 'test', 'unsupervised']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the dataset split names\n",
    "get_dataset_split_names(\"imdb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "efadbb81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    unsupervised: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 50000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the dataset\n",
    "imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b1f6d59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label'],\n",
       "    num_rows: 25000\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the training dataset\n",
    "ds_train = imdb['train']\n",
    "ds_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2847210a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'I love sci-fi and am willing to put up with a lot. Sci-fi movies/TV are usually underfunded, under-appreciated and misunderstood. I tried to like this, I really did, but it is to good TV sci-fi as Babylon 5 is to Star Trek (the original). Silly prosthetics, cheap cardboard sets, stilted dialogues, CG that doesn\\'t match the background, and painfully one-dimensional characters cannot be overcome with a \\'sci-fi\\' setting. (I\\'m sure there are those of you out there who think Babylon 5 is good sci-fi TV. It\\'s not. It\\'s clich√©d and uninspiring.) While US viewers might like emotion and character development, sci-fi is a genre that does not take itself seriously (cf. Star Trek). It may treat important issues, yet not as a serious philosophy. It\\'s really difficult to care about the characters here as they are not simply foolish, just missing a spark of life. Their actions and reactions are wooden and predictable, often painful to watch. The makers of Earth KNOW it\\'s rubbish as they have to always say \"Gene Roddenberry\\'s Earth...\" otherwise people would not continue watching. Roddenberry\\'s ashes must be turning in their orbit as this dull, cheap, poorly edited (watching it without advert breaks really brings this home) trudging Trabant of a show lumbers into space. Spoiler. So, kill off a main character. And then bring him back as another actor. Jeeez! Dallas all over again.',\n",
       " 'label': 0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the test dataset\n",
    "ds_test = imdb['test']\n",
    "ds_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d8ad6f73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label'],\n",
       "    num_rows: 50000\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Concatenate the training and test datasets\n",
    "ds = concatenate_datasets([ds_train, ds_test])\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3c24a2e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label'],\n",
       "    num_rows: 50000\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the unsupervised dataset\n",
    "ds_unsupervised = imdb['unsupervised']\n",
    "ds_unsupervised"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce7eff4",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fd4ef661",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing texts: 0\n",
      "Number of missing labels: 0\n"
     ]
    }
   ],
   "source": [
    "# use pandas to check for missing labels\n",
    "df = pd.DataFrame(ds)\n",
    "num_missing_texts = df['text'].isna().sum()\n",
    "print(f'Number of missing texts: {num_missing_texts}')\n",
    "\n",
    "num_missing_labels = df['label'].isna().sum()\n",
    "print(f'Number of missing labels: {num_missing_labels}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "431f2cf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    50000.000000\n",
       "mean       231.156940\n",
       "std        171.343997\n",
       "min          4.000000\n",
       "25%        126.000000\n",
       "50%        173.000000\n",
       "75%        280.000000\n",
       "max       2470.000000\n",
       "Name: length, dtype: float64"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['length'] = df['text'].apply(lambda x: len(x.split()))\n",
    "df['length'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c680f64c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I rented I AM CURIOUS-YELLOW from my video sto...</td>\n",
       "      <td>0</td>\n",
       "      <td>288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"I Am Curious: Yellow\" is a risible and preten...</td>\n",
       "      <td>0</td>\n",
       "      <td>214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>If only to avoid making this type of film in t...</td>\n",
       "      <td>0</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This film was probably inspired by Godard's Ma...</td>\n",
       "      <td>0</td>\n",
       "      <td>118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Oh, brother...after hearing about this ridicul...</td>\n",
       "      <td>0</td>\n",
       "      <td>311</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label  length\n",
       "0  I rented I AM CURIOUS-YELLOW from my video sto...      0     288\n",
       "1  \"I Am Curious: Yellow\" is a risible and preten...      0     214\n",
       "2  If only to avoid making this type of film in t...      0      93\n",
       "3  This film was probably inspired by Godard's Ma...      0     118\n",
       "4  Oh, brother...after hearing about this ridicul...      0     311"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e58a0c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the distribution of the review lengths\n",
    "df['length'].plot(kind='hist', bins=100, figsize=(10, 5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5802adc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the distribution of the labels\n",
    "labels_train = pd.Series(ds_train['label'])\n",
    "ax = labels_train.value_counts().sort_index() \\\n",
    "    .plot(kind='bar',\n",
    "          title='Count of Reviews by Sentiment',\n",
    "          figsize=(10, 5))\n",
    "ax.set_xlabel('Sentiment')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77cde64",
   "metadata": {},
   "source": [
    "## Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7f24ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic NLTK preprocessing\n",
    "example_text = ds_train[0]['text']\n",
    "example_text\n",
    "tokens = nltk.word_tokenize(example_text)\n",
    "tagged = nltk.pos_tag(tokens)\n",
    "entities = nltk.chunk.ne_chunk(tagged)\n",
    "entities.pprint()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d81b336",
   "metadata": {},
   "source": [
    "## Model Building"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238b71ef",
   "metadata": {},
   "source": [
    "### Shuffle and split the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e934fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the dataset\n",
    "shuffled_dataset = ds.shuffle(seed=10)\n",
    "\n",
    "# Split the dataset into training, validation, and test sets\n",
    "ds_train = shuffled_dataset.select(range(30000))\n",
    "ds_val = shuffled_dataset.select(range(30000, 40000))\n",
    "ds_test = shuffled_dataset.select(range(40000, 50000))\n",
    "\n",
    "#ds_train[0]\n",
    "#ds_val[0]\n",
    "ds_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c02420",
   "metadata": {},
   "source": [
    "### Use the VADER model as a baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15fd8e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VADER (Valence Aware Dictionary and Sentiment Reasoner), ignoring contexts\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "sia.polarity_scores(example_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8bdcd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the polarity score on the entire training dataset\n",
    "results_train = {}\n",
    "counter = 0\n",
    "for row in tqdm(ds_train, total=len(ds_train)):\n",
    "    text = row['text']\n",
    "    results_train[counter] = sia.polarity_scores(text)\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6492a603",
   "metadata": {},
   "outputs": [],
   "source": [
    "vaders = pd.DataFrame(results_train).T\n",
    "vaders = vaders.reset_index().rename(columns={'index': 'Id'})\n",
    "vaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeded193",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.DataFrame(ds_train)\n",
    "df_train = df_train.reset_index().rename(columns={'index': 'Id'})\n",
    "vaders = vaders.merge(df_train, how='left')\n",
    "vaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e37ce80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot VADER scores by sentiment label\n",
    "ax = sns.barplot(data=vaders, x='label', y='compound')\n",
    "ax.set_title('Compound Score by Sentiment Label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369de4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot VADER sub-scores by sentiment label\n",
    "fig, axs = plt.subplots(1, 3, figsize=(12, 3))\n",
    "sns.barplot(data=vaders, x='label', y='pos', ax=axs[0])\n",
    "sns.barplot(data=vaders, x='label', y='neu', ax=axs[1])\n",
    "sns.barplot(data=vaders, x='label', y='neg', ax=axs[2])\n",
    "axs[0].set_title('Positive')\n",
    "axs[1].set_title('Neutral')\n",
    "axs[2].set_title('Negative')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ec69b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the Roberta pretrained model for sentiment analysis\n",
    "MODEL = f\"cardiffnlp/twitter-roberta-base-sentiment\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea0b775",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the example text through the Roberta model and get the scores\n",
    "encoded_text = tokenizer(example_text, return_tensors='pt')\n",
    "output = model(**encoded_text)\n",
    "scores = output[0][0].detach().numpy()\n",
    "scores = softmax(scores)\n",
    "scores_dict = {\n",
    "    'roberta_neg' : scores[0],\n",
    "    'roberta_neu' : scores[1],\n",
    "    'roberta_pos' : scores[2]\n",
    "}\n",
    "scores_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2dd7374b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a function to run the Roberta model on each entry\n",
    "def polarity_scores_roberta(example):\n",
    "    # Tokenize with truncation\n",
    "    encoded_text = tokenizer(\n",
    "        example, \n",
    "        return_tensors='pt', \n",
    "        truncation=True, \n",
    "        padding=True, \n",
    "        max_length=512\n",
    "    )\n",
    "    output = model(**encoded_text)\n",
    "    scores = output[0][0].detach().numpy()\n",
    "    scores = softmax(scores)\n",
    "    print(scores)\n",
    "    scores_dict = {\n",
    "        'roberta_neg': scores[0],\n",
    "        'roberta_neu': scores[1],\n",
    "        'roberta_pos': scores[2]\n",
    "    }\n",
    "    return scores_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c4cfcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the function on the example text\n",
    "polarity_scores_roberta(example_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912c5386",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the Roberta model on the training dataset\n",
    "results_train_both = {}\n",
    "counter = 0\n",
    "\n",
    "for row in tqdm(ds_train, total=len(ds_train)):\n",
    "    try:\n",
    "        text = row['text']\n",
    "        vader_result = sia.polarity_scores(text)\n",
    "        vader_result_rename = {}\n",
    "        for key, value in vader_result.items():\n",
    "            vader_result_rename[f\"vader_{key}\"] = value\n",
    "        roberta_result = polarity_scores_roberta(text)\n",
    "        both = {**vader_result_rename, **roberta_result}\n",
    "        results_train_both[counter] = both\n",
    "        counter += 1\n",
    "    except RuntimeError:\n",
    "        print(f'Broke for row {counter}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23f7376",
   "metadata": {},
   "outputs": [],
   "source": [
    "# try to run the model on Mac GPUs\n",
    "\n",
    "#from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
    "\n",
    "# Check if MPS (Metal Performance Shaders) is available\n",
    "device = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ead1ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer and model\n",
    "model = model.to(device)\n",
    "\n",
    "# Tokenize the input\n",
    "inputs = tokenizer(example_text, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "\n",
    "# Run inference\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "\n",
    "# Get predictions\n",
    "predicted_class = torch.argmax(logits, dim=1).item()\n",
    "print(example_text)\n",
    "print(f\"Predicted class: {predicted_class}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2249bb28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a function to run the Roberta model on the GPUs\n",
    "def polarity_scores_roberta_gpu(example):\n",
    "    # Tokenize with truncation\n",
    "    encoded_text = tokenizer(\n",
    "        example, \n",
    "        return_tensors='pt', \n",
    "        truncation=True, \n",
    "        padding=True, \n",
    "        max_length=512\n",
    "    ).to(device)\n",
    "    \n",
    "    # Run inference\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**encoded_text)\n",
    "        #logits = outputs.logits\n",
    "\n",
    "    # Get predictions\n",
    "    scores = outputs[0][0].detach().cpu().numpy()\n",
    "    scores = softmax(scores)\n",
    "    #print(scores)\n",
    "    scores_dict = {\n",
    "        'roberta_neg': scores[0],\n",
    "        'roberta_neu': scores[1],\n",
    "        'roberta_pos': scores[2]\n",
    "    }\n",
    "    return scores_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af088ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the function on the example text\n",
    "polarity_scores_roberta_gpu(example_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d305f26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the Roberta model on the training dataset using GPUs\n",
    "results_train_both = {}\n",
    "counter = 0\n",
    "\n",
    "for row in tqdm(ds_train, total=len(ds_train)):\n",
    "    try:\n",
    "        text = row['text']\n",
    "        vader_result = sia.polarity_scores(text)\n",
    "        vader_result_rename = {}\n",
    "        for key, value in vader_result.items():\n",
    "            vader_result_rename[f\"vader_{key}\"] = value\n",
    "        roberta_result = polarity_scores_roberta_gpu(text)\n",
    "        both = {**vader_result_rename, **roberta_result}\n",
    "        results_train_both[counter] = both\n",
    "        counter += 1\n",
    "    except RuntimeError:\n",
    "        print(f'Broke for row {counter}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0306ea20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the Roberta model with batch input using GPUs\n",
    "def polarity_scores_roberta_gpu_batch(texts: List[str], batch_size: int = 256) -> Dict[int, Dict[str, float]]:\n",
    "    \"\"\"\n",
    "    Analyze sentiment for a batch of texts.\n",
    "        \n",
    "    Args:\n",
    "        texts (List[str]): List of input texts\n",
    "        batch_size (int): Size of the batch to process\n",
    "            \n",
    "    Returns:\n",
    "        Dict[int, Dict[str, float]]: Dictionary of dictionaries containing sentiment probabilities\n",
    "    \"\"\"\n",
    "\n",
    "    scores_dict = {}\n",
    "    num_batches = math.ceil(len(texts) / batch_size)\n",
    "         \n",
    "    for i in tqdm(range(num_batches)):\n",
    "        # Get the batch of texts\n",
    "        batch_texts = texts[i*batch_size : (i+1)*batch_size]\n",
    "        \n",
    "        # Tokenize with truncation\n",
    "        encoded_text = tokenizer(\n",
    "                 batch_texts, \n",
    "                 return_tensors='pt', \n",
    "                 truncation=True, \n",
    "                 padding=True, \n",
    "                 max_length=512\n",
    "             ).to(device)\n",
    "             \n",
    "        # Run inference\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**encoded_text)\n",
    "            probabilities = torch.nn.functional.softmax(outputs.logits, dim=1)\n",
    "             \n",
    "        # Get predictions\n",
    "        for j, probs in enumerate(probabilities.cpu().numpy()):\n",
    "            idx = i * batch_size + j\n",
    "            scores_dict[idx] = {\n",
    "                     \"roberta_neg\": float(probs[0]),\n",
    "                     \"roberta_neu\": float(probs[1]),\n",
    "                     \"roberta_pos\": float(probs[2])\n",
    "                 }\n",
    "         \n",
    "    return scores_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5739d9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "roberta_batch_result = polarity_scores_roberta_gpu_batch(ds_train['text'])\n",
    "roberta_batch_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5e95f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4adfcb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185ba75d",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(results_train_both).T\n",
    "results_df = results_df.reset_index().rename(columns={'index': 'Id'})\n",
    "results_df = results_df.merge(df_train, how='left')\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4bbbd4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare the VADER and Roberta scores\n",
    "sns.pairplot(data=results_df,\n",
    "             vars=['vader_neg', 'vader_neu', 'vader_pos',\n",
    "                  'roberta_neg', 'roberta_neu', 'roberta_pos'],\n",
    "            hue='label',\n",
    "            palette='tab10')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c390b55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check the VADER and Roberta scores\n",
    "results_df.query('label == 0') \\\n",
    "    .sort_values('roberta_pos', ascending=False)['text'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1abc6ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.query('label == 0') \\\n",
    "    .sort_values('vader_pos', ascending=False)['text'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b260a459",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.query('label == 1') \\\n",
    "    .sort_values('roberta_neg', ascending=False)['text'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91dc112",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.query('label == 1') \\\n",
    "    .sort_values('vader_neg', ascending=False)['text'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45d782e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the Transformers Pipeline for sentiment analysis\n",
    "from transformers import pipeline\n",
    "\n",
    "sent_pipeline = pipeline(\"sentiment-analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "004df64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_pipeline(example_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a1f356",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_pipeline('I love sentiment analysis!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943b6bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_pipeline('I hate having no gpus!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f06fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_pipeline('What a beautiful day! All my clothes got wet!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a86bd94d",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a80604",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a20803",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "30edd616",
   "metadata": {},
   "source": [
    "## Retrieval Augmented Generation (RAG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f93971",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476d6676",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
